{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"u_net_3_2_optimization.ipynb","provenance":[{"file_id":"1iN6N8O99H95gCXPbxDeV_bcWIvA20wyK","timestamp":1622801522113},{"file_id":"1QRbPzUYDfbmMOvf6Bq-XcKu73XeFZDeB","timestamp":1622668251140}],"collapsed_sections":[],"authorship_tag":"ABX9TyMQMUHtA5OZKlAFvFdr3DfP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"51fogsQMGCRv"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcIK8wIsNQYC"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator and then re-execute this cell.')\n","else:\n","    print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3y_W4cSOORH"},"source":["from psutil import virtual_memory\n","\n","ram = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM.'.format(ram))\n","if ram < 20:\n","    print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\", then select High-RAM in the Runtime shape dropdown '\n","          'and then re-execute this cell.')\n","else:\n","    print('You are using a high-RAM runtime.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdsmgeVjDJ9y"},"source":["from tensorflow import config\n","\n","physical_devices = config.list_physical_devices('GPU')\n","try:\n","    config.experimental.set_memory_growth(physical_devices[0], True)\n","except Exception as exception:\n","    print(exception)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLfpAPCcDzYM"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBS0z95JDyn4"},"source":["from tensorflow import device\n","from tensorflow_addons import layers as new_layers\n","from tensorflow.keras import losses\n","from tensorflow.keras import models\n","from tensorflow.keras import optimizers\n","\n","import csv\n","import gc\n","import h5py\n","import numpy as np\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YxdAbGVGiID"},"source":["town = 'Moscow'  #@param ['Berlin', 'Istanbul', 'Moscow']\n","\n","files = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/files/training'.format(town)\n","\n","checkpoints = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/checkpoints/UNet3_2'.format(town)\n","logs = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/logs/UNet3_2/training/logs.csv'.format(town)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKsRLhYg7zdP"},"source":["def get_file_names(files):\n","    file_names = os.listdir(files)\n","    np.random.shuffle(file_names)\n","    return file_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnYokMbaDeAC"},"source":["def get_data(file_path, index):\n","    file = h5py.File(file_path, 'r')\n","    group_key = list(file.keys())[0]\n","    data = np.array(file[group_key][index:index + 72], dtype=np.float32)  # (72, 495, 436, 9)\n","    file.close()\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRvxv67PydA9"},"source":["def get_training_data(file_path, index):\n","    data = get_data(file_path, index)\n","    data = np.take(data, np.arange(8), axis=-1)  # keep only the dynamic channels\n","    data = np.array(np.split(data, 12))  # split in 12 batches of 3 + 3 timestamps\n","    data = np.moveaxis(data, 1, -1).reshape((12, 495, 436, -1))  # combine the timestamps with the channels\n","    np.random.shuffle(data)  # shuffle the batches\n","    data /= 255.0\n","    inputs = data[:, :, :, :24]\n","    outputs = data[:, :, :, 24:]\n","    return inputs, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JEphNWvDdpd"},"source":["with device('gpu:0'):\n","    model = models.load_model(os.path.join(checkpoints, 'model_4.h5'))\n","    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss=losses.mean_squared_error)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqHEamlAHc6A"},"source":["log_file = open(logs, 'a', newline='')\n","log_writer = csv.writer(log_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmzGxtOZX92Q"},"source":["for epoch in range(5, 10):\n","    print('epoch:', epoch)\n","    file_names = get_file_names(files)\n","    for index, file_name in enumerate(file_names):\n","        print('file:', index)\n","        losses = np.zeros(shape=(4,), dtype=np.float64)\n","        for index in range(0, 288, 72):\n","            inputs, outputs = get_training_data(os.path.join(files, file_name), index)\n","            with device('gpu:0'):\n","                history = model.fit(inputs, outputs, epochs=1, batch_size=3)\n","                losses[index // 72] = history.history['loss'][0]\n","        log_writer.writerow([epoch, file_name, np.mean(losses, dtype=np.float64)])\n","        log_file.flush()\n","        gc.collect()\n","    model.save(os.path.join(checkpoints, 'model_{}.h5'.format(epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24oZN5GMqgmY"},"source":["log_file.close()"],"execution_count":null,"outputs":[]}]}