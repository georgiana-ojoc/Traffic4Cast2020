{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"u_net_3.ipynb","provenance":[{"file_id":"1iN6N8O99H95gCXPbxDeV_bcWIvA20wyK","timestamp":1622801522113},{"file_id":"1QRbPzUYDfbmMOvf6Bq-XcKu73XeFZDeB","timestamp":1622668251140}],"collapsed_sections":[],"authorship_tag":"ABX9TyNrw28LcLHXvpBmcWBFT6Ej"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"51fogsQMGCRv"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcIK8wIsNQYC"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator and then re-execute this cell.')\n","else:\n","    print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3y_W4cSOORH"},"source":["from psutil import virtual_memory\n","\n","ram = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM.'.format(ram))\n","if ram < 20:\n","    print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\", then select High-RAM in the Runtime shape dropdown '\n","          'and then re-execute this cell.')\n","else:\n","    print('You are using a high-RAM runtime.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdsmgeVjDJ9y"},"source":["from tensorflow import config\n","\n","physical_devices = config.list_physical_devices('GPU')\n","try:\n","    config.experimental.set_memory_growth(physical_devices[0], True)\n","except Exception as exception:\n","    print(exception)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNE1TiIuVv4a"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBS0z95JDyn4"},"source":["from keras.utils.vis_utils import plot_model\n","from tensorflow import device\n","from tensorflow import image\n","from tensorflow_addons import layers as new_layers\n","from tensorflow.keras import activations\n","from tensorflow.keras import layers\n","from tensorflow.keras import losses\n","from tensorflow.keras import models\n","from tensorflow.keras import optimizers\n","\n","import csv\n","import gc\n","import h5py\n","import numpy as np\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YxdAbGVGiID"},"source":["town = 'Moscow'  #@param ['Berlin', 'Istanbul', 'Moscow']\n","\n","files = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/files/training'.format(town)\n","\n","checkpoints = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/checkpoints/UNet3'.format(town)\n","logs = '/content/gdrive/My Drive/Licenta/Traffic4Cast/{}/logs/UNet3/training/logs.csv'.format(town)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKsRLhYg7zdP"},"source":["def get_file_names(files):\n","    file_names = os.listdir(files)\n","    np.random.shuffle(file_names)\n","    return file_names[:30]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnYokMbaDeAC"},"source":["def get_data(file_path, index):\n","    file = h5py.File(file_path, 'r')\n","    group_key = list(file.keys())[0]\n","    data = np.array(file[group_key][index:index + 72], dtype=np.float32)  # (72, 495, 436, 9)\n","    file.close()\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRvxv67PydA9"},"source":["def get_training_data(file_path, index):\n","    data = get_data(file_path, index)\n","    data = np.take(data, np.arange(8), axis=-1)  # keep only the dynamic channels\n","    data = np.array(np.split(data, 12))  # split in 12 batches of 3 + 3 timestamps\n","    data = np.moveaxis(data, 1, -1).reshape((12, 495, 436, -1))  # combine the timestamps with the channels\n","    np.random.shuffle(data)  # shuffle the batches\n","    data /= 255.0\n","    inputs = data[:, :, :, :24]\n","    outputs = data[:, :, :, 24:]\n","    return inputs, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImIEBQBfk1Ux"},"source":["def get_convolution_block(inputs, filters):\n","    outputs = layers.Conv2D(filters=filters, kernel_size=3, padding='same')(inputs)\n","    outputs = new_layers.GroupNormalization()(outputs)\n","    outputs = layers.Activation(activations.elu)(outputs)\n","\n","    outputs = layers.Conv2D(filters=filters, kernel_size=3, padding='same')(outputs)\n","    outputs = new_layers.GroupNormalization()(outputs)\n","    outputs = layers.Activation(activations.elu)(outputs)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKw8aL05k22b"},"source":["def get_encoder_block(inputs, filters):\n","    outputs = get_convolution_block(inputs, filters)  # skip features\n","    pool = layers.MaxPool2D(pool_size=2, strides=2, padding='same')(outputs)\n","    return outputs, pool"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7d7e4c8k4xz"},"source":["def get_decoder_block(inputs, skip_features, filters):\n","    outputs = layers.Conv2DTranspose(filters=filters, kernel_size=2, strides=2, padding='same')(inputs)\n","    outputs = image.resize(outputs, skip_features.get_shape()[1:3])\n","    outputs = layers.Concatenate()([outputs, skip_features])\n","    outputs = get_convolution_block(outputs, filters)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2PSxAQG9g_v"},"source":["def get_model():\n","    inputs = layers.Input(shape=(495, 436, 24), name='inputs')\n","\n","    encoder_block_1, pool_1 = get_encoder_block(inputs, filters=64)\n","    encoder_block_2, pool_2 = get_encoder_block(pool_1, filters=128)\n","    encoder_block_3, pool_3 = get_encoder_block(pool_2, filters=256)\n","    encoder_block_4, pool_4 = get_encoder_block(pool_3, filters=512)\n","\n","    convolution_block = get_convolution_block(pool_4, 1024)\n","\n","    decoder_block_1 = get_decoder_block(convolution_block, encoder_block_4, 512)\n","    decoder_block_2 = get_decoder_block(decoder_block_1, encoder_block_3, 256)\n","    decoder_block_3 = get_decoder_block(decoder_block_2, encoder_block_2, 128)\n","    decoder_block_4 = get_decoder_block(decoder_block_3, encoder_block_1, 64)\n","\n","    outputs = layers.Conv2D(filters=24, kernel_size=1, padding='same')(decoder_block_4)\n","\n","    return models.Model(inputs=inputs, outputs=outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JEphNWvDdpd"},"source":["with device('gpu:0'):\n","    model = get_model()\n","    model.compile(optimizer=optimizers.Adam(learning_rate=0.0003), loss=losses.mean_squared_error)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hu7fuWD8oADm"},"source":["plot_model(model, to_file='u_net_3.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqHEamlAHc6A"},"source":["file_names = get_file_names(files)\n","\n","log_file = open(logs, 'w', newline='')\n","log_writer = csv.writer(log_file)\n","log_writer.writerow(['epoch', 'file', 'loss'])\n","log_file.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmzGxtOZX92Q"},"source":["for epoch in range(20):\n","    print('epoch:', epoch)\n","    for index, file_name in enumerate(file_names):\n","        print('file:', index)\n","        losses = np.zeros(shape=(4,), dtype=np.float64)\n","        for index in range(0, 288, 72):\n","            inputs, outputs = get_training_data(os.path.join(files, file_name), index)\n","            with device('gpu:0'):\n","                history = model.fit(inputs, outputs, epochs=1, batch_size=3)\n","                losses[index // 72] = history.history['loss'][0]\n","        log_writer.writerow([epoch, file_name, np.mean(losses, dtype=np.float64)])\n","        log_file.flush()\n","        gc.collect()\n","    model.save(os.path.join(checkpoints, 'model_{}.h5'.format(epoch)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24oZN5GMqgmY"},"source":["log_file.close()"],"execution_count":null,"outputs":[]}]}